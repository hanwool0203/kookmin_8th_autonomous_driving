# DQN 기반 Stanley 제어기 파라미터 동적 튜너
**(DQN-based Stanley Controller Parameter Tuner)**

## 1. 개요 (Overview)

이 프로젝트는 전통적인 차량의 횡 방향 제어 알고리즘인 **Stanley Method**에 심층 강화학습(Deep Reinforcement Learning) 기법인 **DQN(Deep Q-Network)**을 접목한 ROS2 패키지입니다.

기존의 Stanley 제어기는 `k`와 같은 핵심 파라미터를 고정된 값으로 사용하기 때문에, 직선 주로, 급격한 코너 등 다양한 주행 환경 모두에서 최적의 성능을 발휘하기 어렵습니다.

본 프로젝트는 이러한 한계를 극복하기 위해, DQN 에이전트를 **상위 레벨의 '파라미터 튜너'**로 사용하여 주행 상황을 실시간으로 분석하고, Stanley 제어기의 파라미터를 동적으로 최적화하여 주행 성능을 극대화하는 것을 목표로 합니다.


## 2. 주요 기능 (Key Features)

* **하이브리드 제어 구조**: 안정적인 Stanley 제어기가 저수준(low-level) 차량 제어를 담당하고, DQN 에이전트가 고수준(high-level)에서 파라미터를 동적으로 튜닝하는 하이브리드 방식을 채택하여 안정성과 적응성을 모두 확보했습니다.
* **실시간 파라미터 최적화**: 에이전트는 주행 중 발생하는 횡방향 오차, 헤딩 오차, 속도 등의 상태를 기반으로 `k` (제어 이득), `heading_weight` (헤딩 가중치), `speed` (속도) 등 핵심 파라미터를 실시간으로 조정합니다.
* **학습/평가 모드 분리**: 학습을 위한 `train` 모드와 학습된 모델의 성능을 순수하게 평가하기 위한 `test` 모드를 분리하여 프로젝트의 사용 목적을 명확히 했습니다.
* **동기식 학습 루프**: ROS2의 비동기 메시지 처리로 인한 데이터 불일치 문제를 해결하기 위해, 센서 데이터(`lane_callback`) 수신을 기점으로 학습 스텝이 진행되는 이벤트 기반 동기식 학습 루프를 구현하여 학습 안정성을 높였습니다.
* **대화형 학습 환경**: 사용자가 키보드('r' 키, 'f' 키)를 통해 에피소드의 시작, 리셋, 긴급 정지를 직접 제어할 수 있어, 디버깅 및 학습 과정 관리가 용이합니다.
* **긴급 정지 기능**: 안전을 위해 'f' 키를 통해 즉시 차량을 정지시키고 모든 학습을 멈추는 기능을 탑재했습니다.
* **학습 모델 저장 및 이어하기**: 학습된 신경망 모델의 가중치를 주기적으로 파일에 저장하고, 재시작 시 자동으로 불러와서 중단된 지점부터 학습을 이어나갈 수 있습니다.


## 3. 프로젝트 아키텍처 (Architecture)

본 시스템은 강화학습 환경과 에이전트가 하나의 ROS2 노드 내에서 유기적으로 동작하도록 설계되었습니다.

1.  **환경 (`StanleyEnv` - ROS2 Node)**:
    * `/lane_point`, `/checkerboard_detect` 등 ROS2 토픽을 통해 센서 데이터를 수신합니다.
    * 수신된 데이터를 가공하여 강화학습에 필요한 **상태(State)**를 정의합니다.
    * 에이전트의 행동 결과에 따라 **보상(Reward)**을 계산합니다.
    * 에피소드의 시작, 진행, 종료(성공/실패)를 관리합니다.
    * `lane_callback` 내에서 전체 학습 또는 평가 루프를 동기식으로 트리거합니다.

2.  **에이전트 (`DQNAgent` - PyTorch)**:
    * 환경으로부터 `상태`를 받아 Epsilon-Greedy 정책(학습 시) 또는 최적 정책(평가 시)에 따라 `행동`(파라미터 조정)을 결정합니다.
    * `경험 리플레이 버퍼`에 자신의 경험을 저장하고, 이를 바탕으로 신경망 모델을 학습시킵니다.
    * `정책망(Policy Network)`과 `타겟망(Target Network)`을 사용하여 안정적으로 학습을 진행합니다.


## 4. 파일 구조 (File Structure)

stanley_controller/
├── stanley_controller/
│   ├── dqn_model.py          # DQN 에이전트의 '뇌' 역할을 하는 신경망 모델 정의
│   ├── replay_buffer.py      # 에이전트의 '경험'을 저장하고 샘플링하는 메모리 버퍼
│   ├── dqn_agent.py          # 모델과 메모리를 사용해 학습과 행동을 총괄하는 에이전트
│   ├── stanley_env.py        # ROS2 환경이자 학습/평가 루프를 포함한 메인 실행 환경
│   ├── train.py              # 학습 환경 노드를 실행시키는 최종 진입점(런처)
│   └── test.py               # 학습된 모델의 성능을 평가하는 최종 진입점(런처)
└── ...

## 5. 시스템 요구사항 (Prerequisites)

* OS: Ubuntu 22.04 (권장)
* ROS2: Humble Hawksbill (권장)
* Python: 3.8+
* PyTorch (CPU 버전): `pip install torch`
* 기타 라이브러리: `numpy`, `matplotlib`, `termios` (`stanley_env.py`의 키보드 리스너에 필요)

> 본 코드는 GPU 없이 **CPU 환경**에서 실행되도록 명시적으로 설정되어 있습니다.


## 6. 설치 방법 (Installation)

```bash
# 1. ROS2 워크스페이스의 src 디렉토리로 이동
cd ~/your_ros2_ws/src

# 2. 프로젝트 클론
git clone <repository_url> stanley_controller

# 3. 워크스페이스 루트로 이동하여 빌드
cd ~/your_ros2_ws
colcon build --packages-select stanley_controller

# 4. 워크스페이스 환경 설정
source install/setup.bash

## 7. 실행 방법 (How to Use)

1.  **센서 노드 실행**: 시뮬레이터 또는 실제 차량의 노드를 실행하여 `/lane_point` 토픽과 `/checkerboard_detect` 토픽이 발행되도록 준비합니다.

2.  **모드별 실행**: 목적에 따라 아래의 명령어를 선택하여 실행합니다.

    * **학습 모드 (Training Mode):**
        새로운 전략을 학습하거나 기존 모델을 추가로 학습시킬 때 사용합니다.
        ```bash
        ros2 run stanley_controller train
        ```

    * **평가 모드 (Test Mode):**
        저장된 모델을 불러와 학습 없이 순수한 주행 성능을 평가할 때 사용합니다.
        ```bash
        ros2 run stanley_controller test
        ```

3.  **수동 제어 (Manual Control)**: 실행된 터미널 창을 활성화하고 키보드로 제어합니다.
    * **`r` 키**: 새로운 에피소드를 시작하거나 현재 에피소드를 강제로 리셋합니다.
    * **`f` 키**: 긴급 정지 기능입니다. 즉시 차량을 멈추고 모든 학습 활동을 중단합니다. 'r' 키를 눌러 리셋하기 전까지 정지 상태가 유지됩니다.

## 8. 향후 개선 과제 (Future Work)

* **자동 리셋 기능**: 키보드 입력 대신 ROS2 서비스를 이용하여 에피소드 종료 시 자동으로 차량을 리셋하는 기능 추가.
* **행동 공간 개선**: 여러 파라미터를 동시에 조절하는 조합 행동을 정의하거나, SAC/DDPG 등 연속적인 행동 공간을 다루는 알고리즘 도입.
* **상태 공간 확장**: 과거 몇 스텝의 상태 정보를 함께 사용(stacked frames)하여 동적인 상황 변화에 더 잘 대응하도록 개선.
* **하이퍼파라미터 관리**: `YAML` 파일을 통해 강화학습 및 제어 관련 하이퍼파라미터를 관리하여 실험의 재현성과 편의성 증대.
